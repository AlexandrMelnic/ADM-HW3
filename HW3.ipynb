{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import re\n",
    "import csv\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part was used for downloading all the movies and saving them into single html files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies2.html'\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "# download all the html pages\n",
    "for i in range(1):\n",
    "    link = str(soup.select('a')[i].text)\n",
    "    time.sleep(np.random.randint(1,6))\n",
    "    movie = str(BeautifulSoup(requests.get(link).text, 'html.parser'))\n",
    "    f = open('C:/Users/Sasha/Desktop/DS/ADM/HW3/movies/movie_{}.html'.format(i), 'w', encoding=\"utf-8\")\n",
    "    f.write(movie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some functions needed to cleare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function needed to remove the quotations, in the form [.], from a string \n",
    "def remove_quotes(x):\n",
    "    # using regex, finding all substrings contained between the characters '[' and ']'\n",
    "    #'quotes' is a list with all those characters, we loop over all of them, and then delete them from 'x'\n",
    "    quotes = re.findall(r'\\[(.+?)\\]', x)\n",
    "    for i in range(len(quotes)):\n",
    "        x = x.replace('[{}]'.format(quotes[i]), '')\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function splits 2 words if they are attached and the second one starts with an uppercase. \n",
    "# this happens many times when there are more than 1 names in 'music', 'starring'...\n",
    "def split_names(a):\n",
    "    for i in range(1,len(a)):\n",
    "        # when we encounter an upper case letter and the previous is not a tab, we split the word\n",
    "        if (a[i].isupper() == True) & (a[i-1] != ' '):\n",
    "            a = a[:i] + ' ' + a[i:]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function neede to get the movie title from the wikipedia page title\n",
    "def get_title(title):\n",
    "    # titles can come in this format [...] - (1987 American film) - Wikipedia\n",
    "    # we clean the title from the final word and everything in the brackets.\n",
    "    bra = re.findall(r'\\((.+?)\\)', title)\n",
    "    for i in range(len(bra)):\n",
    "         title= title.replace(' ({})'.format(bra[i]), '')\n",
    "    title = title[:-12]\n",
    "    return title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting info and making tsv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Here we get the information about the movie. For the intro we select all paragraphs until the first 'h2' tag. For the plot we select all paragraphs between the first and second 'h2' tag. In this way we should get always the first 2 sections of the wikipedia page. The texts in the section are then saved in the string variables 'intro' and 'plot'. \n",
    "Then we get the infobox informations. The infobox table has class 'infobox vevent', so we get that, we analyse all 'keys' of the table: Directed by, Produced by, Starring..., which elements are the respective names. The tsv files are saved as requested and by adding a last element that is the link to the web page, that will be needed for the search engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status:  11000\n",
      "Status:  12000\n",
      "Status:  13000\n",
      "Status:  14000\n",
      "Status:  15000\n",
      "Status:  16000\n",
      "Status:  17000\n",
      "Status:  19000\n",
      "Status:  20000\n"
     ]
    }
   ],
   "source": [
    "# open the html files with BeautifulSoup\n",
    "for j in range(10000):\n",
    "    movie = open('movies2/movie_{}.html'.format(j-10001), 'r', encoding = 'utf-8')\n",
    "    movie = BeautifulSoup(movie, 'html.parser')\n",
    "    # wiki page title\n",
    "    title = str(movie.title.text)\n",
    "    # link of the page\n",
    "    link = movie.find('link', {'rel':'canonical'}).get('href')\n",
    "    # in many cases the information needed are containde in the following div\n",
    "    movie = movie.find('div', class_='mw-parser-output') \n",
    "    \n",
    "    # few pages are disambiguations, if we get on of those we skip the iteraction of the for loop\n",
    "    if title.find('(disambiguation)') != -1:\n",
    "        continue \n",
    "        \n",
    "    # get the title of the movie\n",
    "    title = get_title(title)\n",
    "    \n",
    "    if movie is not None:\n",
    "        # find_next() function gives the next element, that can be a 'p', a 'div', 'a', etc ( when not \n",
    "        # specified). The variable 'x' represents each of those elements, \n",
    "        # and we take them all if their name is 'p', and until we meet the element with name 'h2'\n",
    "        \n",
    "        # here we get the intro\n",
    "        x = movie.p #this is the first paragraph\n",
    "        if x is None:\n",
    "            intro = 'NA'\n",
    "        else:\n",
    "            intro = ''\n",
    "            while x.name != 'h2':\n",
    "                if x.name == 'p':\n",
    "                    #print(x.text)\n",
    "                    intro = str(intro) + str(x.text)\n",
    "                x = x.find_next()\n",
    "                \n",
    "        # here we get the plot\n",
    "        x = x.find_next('p')\n",
    "        if x is None:\n",
    "            plot = 'NA'\n",
    "        else:\n",
    "            plot = ''\n",
    "            while x.name != 'h2':\n",
    "                if x.name == 'p':\n",
    "                    #print(x.text)\n",
    "                    plot = plot + str(x.text)\n",
    "                x = x.find_next()\n",
    "                \n",
    "        # remove quotes symbols, [.], from the intro\n",
    "        intro = remove_quotes(intro)               \n",
    "        #### Now we extract the information from the Infobox ####\n",
    "        # ge the infobox\n",
    "        infobox = movie.find('table', class_ = 'infobox vevent') \n",
    "       \n",
    "        # we check info only if the infobox has some information\n",
    "        # we proceed only if infobox exists\n",
    "        if infobox is not None:\n",
    "            if len(infobox.find_all('th')) != 0:\n",
    "                # we initialize all the variables associated with the respective infos. If the infobox doesn't contain\n",
    "                # the information, then it just remains 'NA'.\n",
    "                director = producer = writer = starring = music = release = runtime = country = language = budget = 'NA'\n",
    "                film_name = infobox.th.text # the first 'th' element is the title'\n",
    "                i = 0\n",
    "                # the 'th' tags contain the 'key' (Directed by, Produced by, etc), and the 'td' the respective names.\n",
    "                for th in infobox.find_all('th'):\n",
    "                    if i <= len(infobox.find_all('td')) - 1:\n",
    "                        info = infobox.find_all('td')[i].text\n",
    "                        if th.text == 'Directed by':\n",
    "                            director = info\n",
    "                        elif th.text == 'Produced by':\n",
    "                            producer = info\n",
    "                        elif th.text == 'Written by':\n",
    "                            writer = info\n",
    "                        elif th.text == 'Starring':\n",
    "                            starring = info\n",
    "                        elif th.text == 'Music by':\n",
    "                            music = info\n",
    "                        elif th.text == 'Release date':\n",
    "                            release = info\n",
    "                        elif th.text == 'Running time':\n",
    "                            runtime = info\n",
    "                        elif th.text == 'Country':\n",
    "                            country = info\n",
    "                        elif th.text == 'Language':\n",
    "                            language = info\n",
    "                        elif th.text == 'Budget':\n",
    "                            budget = info\n",
    "                        i += 1\n",
    "        output_file = title + '\\t' + intro + '\\t'+plot+'\\t'+director+'\\t'+producer+'\\t'+writer+'\\t'+ starring\n",
    "        output_file = output_file + '\\t'+music+'\\t'+release+'\\t'+runtime+'\\t'+country+'\\t'+language+'\\t'+budget\n",
    "        output_file = output_file + '\\t' + link\n",
    "        # we cleare the output_file from the '\\n' and quotes, in form [.].\n",
    "        output_file = output_file.replace('\\n', '')\n",
    "        output_file = remove_quotes(output_file)\n",
    "        output_file = split_names(output_file)\n",
    "        with open('tsv_files2/movie_{}.tsv'.format(j), \"w\", encoding = 'utf-8') as outputter:\n",
    "            outputter.write(output_file)\n",
    "        if j % 1000 == 0:\n",
    "            print ('Status: ',j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports from the nltk library needed to preprocess the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer \n",
    "import collections\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the processing function, that from a string will put all letters in lowercase, remove stop words and do the lemming. The last 2 steps are done by using the library nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing a string, deleting stop words, and lemming\n",
    "# this is needed for making the \n",
    "def processing(string):\n",
    "    # we use the library nltk to remove stop words and do the lemming\n",
    "    # convert all in lower case\n",
    "    string = string.lower()\n",
    "    # delete all but alphanumeric characters\n",
    "    string = re.sub(r'\\W+', ' ', string)\n",
    "    # ge the set of stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(string) \n",
    "    # return string without stop words and after lemming\n",
    "    return np.array(list([PorterStemmer().stem(x) for x in word_tokens if not x in stop_words]))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use 2 dictionaries for the vocabulary and then inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverted index and vocabulary dictionaries\n",
    "inv_index = {}\n",
    "voc = {}\n",
    "# counter for the words in the vocabulary\n",
    "j = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the vocabulary. We take all the elements from the tsv files, except the link to build the vocabulary, since all those informations will be needed for the next search engines. The vocabulary will have as keys the words and as values the respective ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10000):\n",
    "    # open the tsv file\n",
    "    try:\n",
    "        movie_input = list(pd.pandas.read_csv('tsv_files2/movie_{}.tsv'.format(i), encoding='utf-8', quotechar='\"', delimiter='\\t'))\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "\n",
    "    movie = ' '.join(movie_input[0:-1])\n",
    "    # do the set since we are not interested in how many times a word appears\n",
    "    filt_movie = list(set(processing(movie)))\n",
    "    \n",
    "    # loop on the words of the filt_movie array\n",
    "    for word in filt_movie:\n",
    "        # if the word  is not in voc alredy we add it\n",
    "        if word not in voc.keys():\n",
    "            voc[word] = j\n",
    "            # then we increase the j, which is the word id\n",
    "            j += 1\n",
    "\n",
    "    if i % 1000 ==0:\n",
    "        print('Status: ',i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the first index, we take only the first 3 elements of the tsv files, that are : title, intro, plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tsv file\n",
    "for i in range(10000):\n",
    "    # open the tsv file\n",
    "    try:\n",
    "        movie_input = list(pd.pandas.read_csv('tsv_files2/movie_{}.tsv'.format(i), encoding='utf-8', quotechar='\"', delimiter='\\t'))\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "        \n",
    "    # for the first part we only need title intro and plot\n",
    "    # we can join the title, the intro and the plot in the same string\n",
    "    movie = ' '.join(movie_input[0:3])\n",
    "    filt_movie = list(set(processing(movie)))\n",
    "        \n",
    "    # loop on the words of the filt_movie array\n",
    "    for word in filt_movie:\n",
    "        \n",
    "        # check if the word id is in the keys, if it is update with the new document id       \n",
    "        if voc[word] in inv_index.keys():\n",
    "            inv_index[voc[word]].append(i)\n",
    "        # if the word is not in the inv_index yet then add it \n",
    "        else:\n",
    "            inv_index[voc[word]] = [i]\n",
    "          \n",
    "    if i % 1000 == 0:\n",
    "            print('Status: ',i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the second index. Here we use the tfidf score: $$ tfidf_d = tf_dlog(N/td) $$ where $td_d$ is the term frequency for a given document, $N$ is the number of total documents, and $td$ is the number of documents in which that word appears.For every word from the movies files we count how many times it appears in the document, and divide that by the total number of words in the document, getting the tf term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tsv file\n",
    "for i in range(0,10000):\n",
    "    # open the tsv file\n",
    "    try:\n",
    "        movie_input = list(pd.pandas.read_csv('data\\\\TSV\\\\movie_{}.tsv'.format(i+1+20000), encoding='utf-8', quotechar='\"', delimiter='\\t'))\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "        \n",
    "    # we again only need title intro and plot\n",
    "    # we can join the title, the intro and the plot in the same string\n",
    "    movie = ' '.join(movie_input[0:3])\n",
    "    filt_movie = list(processing(movie))\n",
    "    #number of words present for the movie\n",
    "    l = len(filt_movie)    \n",
    "    # loop on the words of the filt_movie array\n",
    "    for word in list(set(filt_movie)):\n",
    "        # index of the word in the array voc\n",
    "        \n",
    "        #number of times the word present in the movie file\n",
    "        n = list(filt_movie).count(word) \n",
    "        #calculating term frequency \n",
    "        tf = round(n/l,3)\n",
    "        if voc[word] in inv_index.keys():\n",
    "            #appending document id and term frequency of the word into the inv_index dictionary\n",
    "            inv_index[voc[word]].append((i,tf))\n",
    "        else:\n",
    "            inv_index[voc[word]] = [(i,tf)]\n",
    "            \n",
    "    if i % 100 == 0:\n",
    "            print('Status: ',i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the idf term, and make the complete normalized tfidf term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('index1.json', 'r') as myfile1:\n",
    "    data1=myfile1.read()\n",
    "inv_index = json.loads(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total documents ids\n",
    "docs = []\n",
    "for x in inv_index.values():\n",
    "    for y in x:\n",
    "        docs.append(y[0])\n",
    "docs = list(set(docs))\n",
    "# number of total documents\n",
    "N = len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the idf term is the same for all documents of the respective word\n",
    "for word_id in inv_index:\n",
    "    # first we count the number of documents the word appears in \n",
    "    cont = 0\n",
    "    for doc in inv_index[word_id]:\n",
    "        cont += 1\n",
    "    # then evaluate the idf term  \n",
    "    idf = np.log(N/cont)\n",
    "    #print(word_id, idf)\n",
    "    # then update the inv_index with the tfidf terms.\n",
    "    for doc in inv_index[word_id]:\n",
    "        doc[1] *= idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the document id we evaluate its norm\n",
    "def norm (doc_id):\n",
    "    norm = 0\n",
    "    for word_id in inv_index:\n",
    "        for doc in inv_index[word_id]:\n",
    "            if doc[0] == doc_id:\n",
    "                norm += doc[1]*doc[1]\n",
    "    return math.sqrt(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use an alternative quantity to the term frequency, the maximum tf normalization. This last one is given by : $$ ntf_d =  a+(1-a)\\frac{tf_d}{\\text{max}_{tf}(tf_d)} $$ \n",
    "where, $a$ is an arbitrary value usually set to 0.4, $tf_d$ the term frequency for that document, and $\\text{max}_{tf}(tf_d)$ is the maximum $tf$ component of the document. This term is then multplied for the respective $idf$. The code is similar to the previous index, except that instead of evaluating the index only on the intro and plot we do it on all the intormations, including directors, producers, starring..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the tsv file\n",
    "for i in range(0,10000):\n",
    "    # open the tsv file\n",
    "    try:\n",
    "        movie_input = list(pd.pandas.read_csv('data\\\\TSV\\\\movie_{}.tsv'.format(i+1+20000), encoding='utf-8', quotechar='\"', delimiter='\\t'))\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "        \n",
    "    # we again only need title intro and plot\n",
    "    # we can join the title, the intro and the plot in the same string\n",
    "    movie = ' '.join(movie_input[0:-1])\n",
    "    filt_movie = list(processing(movie))\n",
    "    #number of words present for the movie\n",
    "    l = len(filt_movie)    \n",
    "    # loop on the words of the filt_movie array\n",
    "    for word in list(set(filt_movie)):\n",
    "        # index of the word in the array voc\n",
    "        \n",
    "        #number of times the word present in the movie file\n",
    "        n = list(filt_movie).count(word) \n",
    "        #calculating term frequency \n",
    "        tf = round(n/l,3)\n",
    "        if voc[word] in inv_index.keys():\n",
    "            #appending document id and term frequency of the word into the inv_index dictionary\n",
    "            inv_index[voc[word]].append((i,tf))\n",
    "        else:\n",
    "            inv_index[voc[word]] = [(i,tf)]\n",
    "            \n",
    "    if i % 100 == 0:\n",
    "            print('Status: ',i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the complete ntfidf term and normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocabulary.json', 'r') as myfile:\n",
    "    data=myfile.read()\n",
    "#vocabulary\n",
    "voc = json.loads(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the first index here. Given an input query we just search for all documents that have all those words in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "america love story\n"
     ]
    }
   ],
   "source": [
    "# here is the input query, after that we immediatly preprocess it\n",
    "query = input()\n",
    "query = processing(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we append in result list all documents that have he query words\n",
    "result = []\n",
    "# loop on the query words\n",
    "for word in query:\n",
    "    # earch row of result corresponds to a word of the query\n",
    "    w = []\n",
    "    #  check the id of the word\n",
    "    word_id = str(voc[word])\n",
    "    for doc in inv_index[word_id]:\n",
    "    # then check the documents corresponding to that id\n",
    "        w.append(doc[0])\n",
    "    result.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we are interested in documents in which all the words appear we do the intersection\n",
    "# between all rows of result\n",
    "a = set(result[0])\n",
    "for i in range(1,len(result)):\n",
    "    #a = set(result[i])\n",
    "    a = a.intersection(set(result[i]))\n",
    "result = list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toy Story 3</td>\n",
       "      <td>Toy Story 3 is a  2010 American computer-anim...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Toy_Story_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Blue Butterfly</td>\n",
       "      <td>The Blue Butterfly ( French: Le papillon bleu...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Blue_Butterfly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The American President</td>\n",
       "      <td>The American President is a 1995 American rom...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_American_Pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>When Harry Met Sally...</td>\n",
       "      <td>When Harry Met Sally... is a 1989 American ro...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/When_Harry_Met_S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American Beauty</td>\n",
       "      <td>American Beauty is a 1999 American drama film...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/American_Beauty_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Title                                              Intro  \\\n",
       "0              Toy Story 3   Toy Story 3 is a  2010 American computer-anim...   \n",
       "1       The Blue Butterfly   The Blue Butterfly ( French: Le papillon bleu...   \n",
       "2   The American President   The American President is a 1995 American rom...   \n",
       "3  When Harry Met Sally...   When Harry Met Sally... is a 1989 American ro...   \n",
       "4          American Beauty   American Beauty is a 1999 American drama film...   \n",
       "\n",
       "                                                Link  \n",
       "0          https://en.wikipedia.org/wiki/Toy_Story_3  \n",
       "1   https://en.wikipedia.org/wiki/The_Blue_Butterfly  \n",
       "2  https://en.wikipedia.org/wiki/The_American_Pre...  \n",
       "3  https://en.wikipedia.org/wiki/When_Harry_Met_S...  \n",
       "4  https://en.wikipedia.org/wiki/American_Beauty_...  "
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the documents in output dictionary, given the document id we import the tsv file to get the intro\n",
    "# and the link to the web page\n",
    "output = {'Title':[], 'Intro':[], 'Link':[]}\n",
    "for i in range(len(result)):\n",
    "    if (result[i] >=10000) & (result[i]< 20000):\n",
    "        movie = list(pd.pandas.read_csv('tsv_files2/movie_{}.tsv'.format(result[i]), encoding='utf-8', quotechar='\"', delimiter='\\t'))\n",
    "        output['Title'].append(movie[0])\n",
    "        output['Intro'].append(movie[1])\n",
    "        output['Link'].append(movie[-1])\n",
    "pd.DataFrame(output).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the second index with the tfidf scores. Given an input query this can be seen as n-dimensional vector (with n the number of words in input) in the space of words. The components of the query vector are all zeros except those corresponding to the input words, which components are all equal to 1/sqrt(n) (the vector is normalized). We use here the cosine similarity to check the \"distance\" between the query and all our documents, and at the end we print the documents which have the greater similarity with the input query. Every document in the space of words has components that are its tfidf terms, so given this we just need to compute the dot product between 2 vectors and their norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the function that returns the similarity\n",
    "\n",
    "# the document vector is alredy normalized, so we just need to compute the dot product between the query \n",
    "# vector and the document\n",
    "def calc_cos (query, doc_id):\n",
    "    dot_p = 0\n",
    "    for words in query:\n",
    "        word_id = str(voc[words])\n",
    "        for docs in inv_index[word_id]:\n",
    "            if docs[0] == doc_id:\n",
    "                dot_p += docs[1]\n",
    "    return(dot_p/math.sqrt(len(query)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "music love gun war\n"
     ]
    }
   ],
   "source": [
    "# here is the input query, after that we immediatly preprocess it\n",
    "query = input()\n",
    "query = processing(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the improve the search time we apply initially the first search engine on the new index\n",
    "\n",
    "# we append in result list all documents that have the query words\n",
    "result = []\n",
    "for word in query:\n",
    "    w = []\n",
    "    # first check the id\n",
    "    word_id = str(voc[word])\n",
    "    # then check the documents corresponding to that id\n",
    "    for docs in inv_index[word_id]:\n",
    "        w.append(docs[0])\n",
    "    result.append(w)\n",
    "#result = list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set(result[0])\n",
    "for i in range(1,len(result)):\n",
    "    #a = set(result[i])\n",
    "    a = a.intersection(set(result[i]))\n",
    "result = list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "20\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "result = list(a)\n",
    "i=0\n",
    "for doc in list(a):\n",
    "    norm(doc)\n",
    "    i+=1\n",
    "    if i % 10==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then save in a dictionary the documents id and the similarity\n",
    "sim = {'doc_id': [], 'similarity': []}\n",
    "# search only in documents that have the query words\n",
    "for doc in result:\n",
    "    x = calc_cos(query, doc)\n",
    "    if x != 0: \n",
    "        sim['doc_id'].append(doc)\n",
    "        sim['similarity'].append(calc_cos(query,doc)/norm(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6505</td>\n",
       "      <td>0.071870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>18500</td>\n",
       "      <td>0.060172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>717</td>\n",
       "      <td>0.058660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2578</td>\n",
       "      <td>0.058145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>3939</td>\n",
       "      <td>0.054766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id  similarity\n",
       "23    6505    0.071870\n",
       "11   18500    0.060172\n",
       "14     717    0.058660\n",
       "3     2578    0.058145\n",
       "21    3939    0.054766"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(sim).sort_values(by= ['similarity'], ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have all the documents id's and the similarities. Just need to print the desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Link</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A Christmas Story</td>\n",
       "      <td>A Christmas Story is a 1983 American Christma...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/A_Christmas_Story</td>\n",
       "      <td>0.012214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Deal of the Century</td>\n",
       "      <td>Deal of the Century is a 1983 American comedy...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Deal_of_the_Century</td>\n",
       "      <td>0.009160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>National Lampoon's Vacation</td>\n",
       "      <td>National Lampoon's Vacation, sometimes referr...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/National_Lampoon...</td>\n",
       "      <td>0.006107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Reuben, Reuben</td>\n",
       "      <td>Reuben, Reuben is a 1983 comedy-drama film di...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Reuben,_Reuben</td>\n",
       "      <td>0.012214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Right Stuff</td>\n",
       "      <td>The Right Stuff is a 1983 American epic histo...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Right_Stuff_...</td>\n",
       "      <td>0.006107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Title  \\\n",
       "0            A Christmas Story   \n",
       "1          Deal of the Century   \n",
       "2  National Lampoon's Vacation   \n",
       "3               Reuben, Reuben   \n",
       "4              The Right Stuff   \n",
       "\n",
       "                                               Intro  \\\n",
       "0   A Christmas Story is a 1983 American Christma...   \n",
       "1   Deal of the Century is a 1983 American comedy...   \n",
       "2   National Lampoon's Vacation, sometimes referr...   \n",
       "3   Reuben, Reuben is a 1983 comedy-drama film di...   \n",
       "4   The Right Stuff is a 1983 American epic histo...   \n",
       "\n",
       "                                                Link  Similarity  \n",
       "0    https://en.wikipedia.org/wiki/A_Christmas_Story    0.012214  \n",
       "1  https://en.wikipedia.org/wiki/Deal_of_the_Century    0.009160  \n",
       "2  https://en.wikipedia.org/wiki/National_Lampoon...    0.006107  \n",
       "3       https://en.wikipedia.org/wiki/Reuben,_Reuben    0.012214  \n",
       "4  https://en.wikipedia.org/wiki/The_Right_Stuff_...    0.006107  "
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = {'Title':[], 'Intro':[], 'Link':[], 'Similarity':[]}\n",
    "for i in range(len(sim['doc_id'])):\n",
    "    try:\n",
    "        movie = list(pd.pandas.read_csv('tsv_files2/movie_{}.tsv'.format(sim['doc_id'][i]), encoding='utf-8', quotechar='\"', delimiter='\\t'))\n",
    "    except FileNotFoundError:\n",
    "        continue\n",
    "    output['Title'].append(movie[0])\n",
    "    output['Intro'].append(movie[1])\n",
    "    output['Link'].append(movie[-1])\n",
    "    output['Similarity'].append(sim['similarity'][i])\n",
    "pd.DataFrame(output).head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
